# Spark-pipeline

**This project aim for practice pyspark engine**
> *The project will be updated weekly for learning purpose*


## Sprint 1:
### Build a simple data pipeline using pyspark engine 

1. **Extract**: Read file csv from local file system

2. **Transform**: Rename Columns

3. **Load**: Convert pyspark_df to parquet partition file and reload to file system


## Sprint 2:
### Pipeline development
1. **Extract** : 
    * Build reuse pipeline to get data from:
        * [ ] csv
        * [ ] xlsx
        * [ ] API
        * [ ] Web Crawling
        * [ ] Database
        * [ ] NoSQL
        * [ ] Streaming Source ( Kafka )

2. **Transform** : Do some common transform
    * [ ] Add new columns
    * [ ] Updated columns
    * [ ] Handling N/A
    * [ ] Filtering
    * [ ] Handling Duplicate data
    * [ ] Join
3. **Load** : 
    * [ ] Write to file storage ( file system, HDFS, sFTP )
    * [ ] Write partition
    * [ ] Write to database ( mysql, postgre, mssql )
    * [ ] Write to NoSQL ( MongoDB )


